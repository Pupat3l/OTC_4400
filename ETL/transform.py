# -*- coding: utf-8 -*-
"""CIS 4400 - OTC Market.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13yuXwoV-eKkxx2XT1z90MDL1QDbpuBDZ
"""



#!pip install boto3

import pandas as pd
import boto3
from io import StringIO
import os
try:
  import s3fs
except:
  #!pip install s3fs
  i=0

# Set your AWS credentials and S3 bucket information
aws_access_key_id = 'xyz'
aws_secret_access_key = 'xyz'
bucket_name = 'bucket-name'
file_key = 'path/to/data.csv'

# Create an S3 client
s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)

filename="s3/url/path/to/data_csv"
df = pd.read_csv(filename, sep=',', nrows=1000000)
df.columns

col_list = df.columns
col_list

df['VenueID']=df['Venue']
dim_venue=df[['VenueID']]
dim_venue = dim_venue.drop_duplicates(subset=['VenueID'])
dim_venue

#fixing SharesOutstandingAsOfDate
df['SharesOutstandingAsOfDate'] = pd.to_datetime(
    df['SharesOutstandingAsOfDate']
        .astype(str)
        .replace({'inf': 'NaT', '-inf': 'NaT'})
        .replace({'nan': 'NaT'})
        .fillna('NaT'),
    errors='coerce',
    format='%Y%m%d'
)

dim_security=df[['SECID','Symbol','CUSIP','Issue','SecType','Class','CompID','CaveatEmptor','SharesOutstanding','SharesOutstandingAsOfDate']]
dim_security = dim_security.drop_duplicates(subset=['SECID'])
dim_security



dim_security

dim_security.dtypes

dim_tier=df[['TierID','TierName','DAD_PAL']]
dim_tiers = dim_tier.drop_duplicates(subset=['TierName'])
dim_tiers

'SourceID' in col_list

sources=['BestBidSource']
source_df=df[sources]
source_df=source_df.dropna()
source_df

dim_source = source_df.drop_duplicates(subset=['BestBidSource'])
dim_source

'StatusID' in col_list

status=['SecurityStatus']
dim_status=df[status].drop_duplicates(subset=['SecurityStatus'])
dim_status

sample_date=df[['PreviousCloseDate']]

missing_values = sample_date.isna()

missing_values[missing_values['PreviousCloseDate']==True]
#null values are 267225

sample_date=sample_date.dropna()

len(sample_date) + 267225

sample_date['PreviousCloseDate'] = pd.to_datetime(sample_date['PreviousCloseDate'].astype(int).astype(str), errors='coerce', format='%Y%m%d')

sample_date.sort_values('PreviousCloseDate')

sample_date.sort_values('PreviousCloseDate')

sample_date['Year'] = sample_date['PreviousCloseDate'].dt.year.astype(int)
sample_date['Month'] = sample_date['PreviousCloseDate'].dt.month.astype(int)
sample_date['Day'] = sample_date['PreviousCloseDate'].dt.day.astype(int)
sample_date['Weekday'] = sample_date['PreviousCloseDate'].dt.day_name()

sample_date

sample_date.drop_duplicates()

combined_dates=pd.concat([df['ClosingInsideBidPriceDate'],df['ClosingInsideAskPriceDate'],df['PreviousCloseDate'],df['ClosingBestBidDate'],df['ClosingBestAskDate'],df['ShortInterestDate']])

combined_dates

result_df = pd.DataFrame({'DateID': combined_dates})
result_df

result_df.drop_duplicates('DateID')

dim_dates=result_df.drop_duplicates()
dim_dates

missing_values = dim_dates.isna()
missing_values[missing_values['DateID']==True]

dim_dates=result_df.drop_duplicates()
dim_dates
dim_dates=dim_dates.dropna()

dim_dates['DateID'] = pd.to_datetime(dim_dates['DateID'].astype(int).astype(str), errors='coerce', format='%Y%m%d')

dim_dates.sort_values('DateID')

dim_dates['Year'] = dim_dates['DateID'].dt.year.astype(int)
dim_dates['Month'] = dim_dates['DateID'].dt.month.astype(int)
dim_dates['Day'] = dim_dates['DateID'].dt.day.astype(int)
dim_dates['Weekday'] = dim_dates['DateID'].dt.day_name()
dim_dates['Quarter'] = dim_dates['DateID'].dt.quarter
dim_dates['WeekOfMonth'] = dim_dates['DateID'].dt.day.apply(lambda d: (d-1) // 7 + 1)
dim_dates['WeekOfYear'] = dim_dates['DateID'].dt.isocalendar().week
dim_dates['DayOfMonth'] = dim_dates['DateID'].dt.day

dim_dates.sort_values('DateID')

dim_status

dim_source

'''import hashlib

def generate_hash_id(ClosingBestBid, ClosingBestAsk, BestBidSource,BestAskSource):
    # Create a unique string by concatenating the fields
    unique_str = f"{ClosingBestBid}|{ClosingBestAsk}|{BestBidSource}|{BestAskSource}"

    # Apply SHA-256 hash function
    hash_obj = hashlib.sha256(unique_str.encode())
    hash_id = hash_obj.hexdigest()

    return hash_id

for i in range(length):
  ClosingBestBid = dim_source['ClosingBestBid'][i]
  ClosingBestAsk = dim_source['ClosingBestAsk'][i]
  BestBidSource = dim_source['BestBidSource'][i]
  BestAskSource = dim_source['BestAskSource'][i]
  unique_id = generate_hash_id(ClosingBestBid, ClosingBestAsk, BestBidSource, BestAskSource)
  source_a['SourceID'].append(unique_id)

'''

dim_source.info()

dim_status

dim_source.rename(columns={'BestBidSource': 'SourceID'}, inplace=True)
dim_source

dim_status.rename(columns={'SecurityStatus':'StatusID'},inplace=True)
dim_status

dim_security

dim_tiers

dim_venue

dim_security

df.head(1)

facts=['SECID','VenueID','TierID','SecurityStatus','BestAskSource','BestBidSource','ClosingInsideBidPriceDate','ClosingInsideAskPriceDate','PreviousCloseDate','ClosingBestBidDate','ClosingBestAskDate','ShortInterestDate','ClosingInsideBidPrice','ClosingInsideAskPrice','InsideBid_AskMidPrice','OpenPrice','HighPrice','LowPrice','LastPrice','PreviousClosePrice','ShareVolume','DollarVol','TradeCount','OTCLinkDolVol','OTCLinkShareVol','OTCLinkExecCount','ShortIntVol','MMIDCount','BFCMmid','ClosingBestAsk','ClosingBestBid']

df.columns

dates=['ClosingInsideBidPriceDate','ClosingInsideAskPriceDate','PreviousCloseDate','ClosingBestBidDate','ClosingBestAskDate','ShortInterestDate']

for i in dates:
  df[i] = df[i].fillna(0).astype(int)
  df[i] = pd.to_datetime(df[i].astype(int).astype(str),errors='coerce', format='%Y%m%d')

check=df[dates]
check

df[dates]

dim_facts=df[facts]
dim_facts

len(dim_facts.columns)

dim_dates.info()

facts_list=dim_facts.columns.tolist()
facts_list

dim_facts.info()

dim_status

dim_security.info()

dim_tiers

dim_venue

dim_status

dim_source

csv_buffer=StringIO()

dim_security.to_csv(csv_buffer,index=False)

s3_key="s3/path/to/cleaned/files"
s3_bucket_name = "bucket-name"

s3.put_object(Bucket=s3_bucket_name, Key=s3_key, Body=csv_buffer.getvalue())

response = s3.get_object(Bucket=s3_bucket_name, Key=s3_key)
data = response['Body'].read().decode('utf-8')

# %%
status_df=pd.read_csv(StringIO(data))
status_df.info()

status_df.dtypes